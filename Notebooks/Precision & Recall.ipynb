{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Precision & Recall.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CvLlgm9RZ6a"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTAjSwK5RKg0"
      },
      "source": [
        "!pip install transformers -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pk\n",
        "from Embeddings import *\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, r2_score"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6YhDTIUp9ma"
      },
      "source": [
        "text = pd.read_csv('/content/links.csv')['text'].to_list()\n",
        "query = pd.read_csv('/content/query_df.csv')['0'].to_list()\n",
        "\n",
        "prec_recall = pd.read_csv('/content/manualData.csv')\n",
        "prec_recall = prec_recall.drop(['Unnamed: 7', 'Unnamed: 8', 'Relevance1', 'Relevance2', 'Rel3', 'rel4', 'rel5'], \n",
        "                               axis = 1)\n",
        "prec_recall = prec_recall[:20]\n",
        "prec_recall1 = pd.read_csv('/content/manualData1.csv')\n",
        "\n",
        "predicted1 = pd.read_csv('/content/PredictedPrec1.csv')\n",
        "engine = pd.read_csv('/content/EnginePrec1.csv')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsLWr-zxO3Gu"
      },
      "source": [
        "## Fuzzy Logic For Keywords - Naive Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQAb76Cl1LT"
      },
      "source": [
        "### Making New DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ru_OM5nO4FO"
      },
      "source": [
        "columns = []\n",
        "for i in range(len(prec_recall)):\n",
        "    user_query = prec_recall['Query'][i]\n",
        "\n",
        "    scores = []\n",
        "    for que in text:\n",
        "        scores.append(fuzz.ratio(user_query, que))\n",
        "\n",
        "    ranks = np.array(scores).argsort()[-100:][::-1]\n",
        "    reccs = [text[i] for i in ranks]\n",
        "    reccs = list(set(reccs))[-10:]\n",
        "    columns.append(reccs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_WoBVpTO3_4"
      },
      "source": [
        "# pd.DataFrame(columns).to_csv('/content/PredictedPrec.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozu6HAS1l3M8"
      },
      "source": [
        "### Calculating Precision, Recall and F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVSm7LzNkS89"
      },
      "source": [
        "def accuracy_f1_summary(y_test, y_pred):\n",
        "    \n",
        "    # precision tp / (tp + fp)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    \n",
        "    # recall: tp / (tp + fn)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    \n",
        "    # f1: 2 tp / (2 tp + fp + fn)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEA8lyr5YO7E"
      },
      "source": [
        "prec_recall1 = prec_recall1.fillna(0)\n",
        "engine1 = engine1.fillna(0)\n",
        "\n",
        "prec_recall1 = prec_recall1.drop(['Sr.No.', 'Query'], axis = 1)\n",
        "predicted1 = predicted1.drop(['Sr. No.', 'Query', 'Relevance6', 'Relevance7', 'Relevance8', 'Relevance9', 'Relevance10'], axis = 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtM_T2yolxpc",
        "outputId": "efbc6c5f-80ca-4842-c2b9-a1ceda241b16"
      },
      "source": [
        "prec, rec, f11 = [], [], []\n",
        "for i in range(20):\n",
        "    a, b, c = accuracy_f1_summary(prec_recall1.iloc[i], predicted1.iloc[i])\n",
        "    prec.append(a)\n",
        "    rec.append(b)\n",
        "    f11.append(c)\n",
        "\n",
        "# Averages\n",
        "print(f'Average Precision: {sum(prec)/len(prec)}')\n",
        "print(f'Average Recall: {sum(rec)/len(rec)}')\n",
        "print(f'Average F1 Score: {sum(f11)/len(f11)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Precision: 0.6875\n",
            "Average Recall: 0.34750000000000003\n",
            "Average F1 Score: 0.4305555555555555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qebldZz4SeLO"
      },
      "source": [
        "## Recommendation Engine "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hrJLe_ISh6e"
      },
      "source": [
        "### Making new Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpYksmtIShcS"
      },
      "source": [
        "with open('/content/Embedd.pk', 'rb') as f:\n",
        "    embedding = pk.load(f)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaxW2_tTWKhL"
      },
      "source": [
        "columns = []\n",
        "for i in range(len(prec_recall)):\n",
        "    print(f'Query: {i + 1}')\n",
        "    user_query = prec_recall['Query'][i]\n",
        "\n",
        "    we_user = SentenceEmbedding(user_query)\n",
        "    we_user.get_embeddings()\n",
        "\n",
        "    scores = []\n",
        "    for text in embedding:\n",
        "        scores.append(r2_score(we_user.embedding, text))\n",
        "\n",
        "    ranks = np.array(scores).argsort()[-10:][::-1]\n",
        "    reccs = [query[i]for i in ranks[:5]]\n",
        "\n",
        "    columns.append(reccs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFgGfE8-aLHu"
      },
      "source": [
        "pd.DataFrame(columns).to_csv('/content/EnginePrec.csv', index = False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpJbuwt7efjt"
      },
      "source": [
        "### Calculating Precision and Recall\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44t0X-8-eiSe"
      },
      "source": [
        "def accuracy_f1_summary(y_test, y_pred):\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    \n",
        "    # precision tp / (tp + fp)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    \n",
        "    # recall: tp / (tp + fn)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    \n",
        "    # f1: 2 tp / (2 tp + fp + fn)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywQOXOCoeiuN"
      },
      "source": [
        "engine1 = engine.fillna(0)\n",
        "predicted1 = predicted1.fillna(0)\n",
        "\n",
        "engine1 = engine1.drop(['Sr. No.', 'Query'], axis = 1)\n",
        "predicted1 = predicted1.drop(['Sr. No.', 'Query', 'Relevance6', 'Relevance7', 'Relevance8', 'Relevance9', 'Relevance10'], axis = 1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2u6oig-fYks",
        "outputId": "bddb2aa7-3890-4adf-c9aa-fbf538dbe1bf"
      },
      "source": [
        "prec, rec, f11 = [], [], []\n",
        "for i in range(20):\n",
        "    a, b, c = accuracy_f1_summary(engine1.iloc[i], predicted1.iloc[i])\n",
        "    prec.append(a)\n",
        "    rec.append(b)\n",
        "    f11.append(c)\n",
        "\n",
        "# Averages\n",
        "print(f'Average Precision: {sum(prec)/len(prec)}')\n",
        "print(f'Average Recall: {sum(rec)/len(rec)}')\n",
        "print(f'Average F1 Score: {sum(f11)/len(f11)}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Precision: 0.6583333333333334\n",
            "Average Recall: 0.4208333333333333\n",
            "Average F1 Score: 0.4824206349206349\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}